{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3: topic modelling\n",
    "### Катя Такташева"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rec.autos' 'comp.sys.mac.hardware' 'comp.graphics' 'sci.space'\n",
      " 'talk.politics.guns' 'sci.med' 'comp.sys.ibm.pc.hardware'\n",
      " 'comp.os.ms-windows.misc' 'rec.motorcycles' 'talk.religion.misc'\n",
      " 'misc.forsale' 'alt.atheism' 'sci.electronics' 'comp.windows.x'\n",
      " 'rec.sport.hockey' 'rec.sport.baseball' 'soc.religion.christian'\n",
      " 'talk.politics.mideast' 'talk.politics.misc' 'sci.crypt']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "\n",
       "            target_names  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "print(df.target_names.unique())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "\n",
    "def preprocess(text, punct=True, lem=False):\n",
    "    \"\"\"\n",
    "    preprocesses texts\n",
    "    \"\"\"\n",
    "    text = re.sub('\\S*@\\S*\\s?', '', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub(\"\\'\", \"\", text)\n",
    "    if punct == False:\n",
    "        words = []\n",
    "        for word in text.split():\n",
    "            words.append(word.strip(punctuation))\n",
    "        text = ' '.join(words)\n",
    "    if lem == True:\n",
    "        text = ' '.join([i.lemma_ for i in nlp(text)])\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed'] = df['content'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>From: (wheres my thing) Subject: WHAT car is t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>From: (Guy Kuo) Subject: SI Clock Poll - Final...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>From: (Thomas E Willis) Subject: PB questions....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>From: (Joe Green) Subject: Re: Weitek P9000 ? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>From: (Jonathan McDowell) Subject: Re: Shuttle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "\n",
       "            target_names                                       preprocessed  \n",
       "0              rec.autos  From: (wheres my thing) Subject: WHAT car is t...  \n",
       "1  comp.sys.mac.hardware  From: (Guy Kuo) Subject: SI Clock Poll - Final...  \n",
       "2  comp.sys.mac.hardware  From: (Thomas E Willis) Subject: PB questions....  \n",
       "3          comp.graphics  From: (Joe Green) Subject: Re: Weitek P9000 ? ...  \n",
       "4              sci.space  From: (Jonathan McDowell) Subject: Re: Shuttle...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    \"\"\"\n",
    "    creates word lists\n",
    "    \"\"\"\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(df.preprocessed.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = [b for sent in data_words for b in zip(sent[:-1], sent[1:])]\n",
    "freq = nltk.FreqDist(bigrams) \n",
    "fdist = freq.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mallet совсем никак не хочет работать на маке, пробовала и в колабе и на локал хосте :( вообще все пишут, что эта проблема прям очень частая и все решения на десятке сайтов работают у людей с переменным успехом.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command '/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input /var/folders/gs/wv6n0xks3s15ys1bvtd7brq40000gn/T/d8484f_corpus.txt --output /var/folders/gs/wv6n0xks3s15ys1bvtd7brq40000gn/T/d8484f_corpus.mallet' returned non-zero exit status 127.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5d571c656b15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                              \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                              \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                              alpha='auto')\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Show Topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mallet_path, corpus, num_topics, alpha, id2word, workers, prefix, optimize_interval, iterations, topic_threshold, random_seed)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinferencer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \"\"\"\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmallet_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' train-topics --input %s --num-topics %s  --alpha %s --optimize-interval %s '\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;34m'--num-threads %s --output-state %s --output-doc-topics %s --output-topic-keys %s '\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/wrappers/ldamallet.py\u001b[0m in \u001b[0;36mconvert_input\u001b[0;34m(self, corpus, infer, serialize_corpus)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmd\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcorpustxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcorpusmallet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"converting temporary corpus to MALLET format with %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(stdout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input /var/folders/gs/wv6n0xks3s15ys1bvtd7brq40000gn/T/d8484f_corpus.txt --output /var/folders/gs/wv6n0xks3s15ys1bvtd7brq40000gn/T/d8484f_corpus.mallet' returned non-zero exit status 127."
     ]
    }
   ],
   "source": [
    "mallet_path = '/mallet-2.0.8/bin/mallet'\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, \n",
    "                                             corpus=corpus, \n",
    "                                             num_topics=3, \n",
    "                                             id2word=id2word,\n",
    "                                             alpha='auto')\n",
    "\n",
    "# Show Topics\n",
    "pprint(ldamallet.show_topics(formatted=False))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Найдем оптимальное количество топиков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_param(groups: list):\n",
    "    \"\"\"\n",
    "    finds the optimal number of topics\n",
    "    for the given corpora according to\n",
    "    coherence score\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for n in groups:\n",
    "        print('* ' * 30)\n",
    "        print(f'n_topics={n}')\n",
    "        print('Extracting topics...')\n",
    "        lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                    id2word=id2word,\n",
    "                                                    num_topics=n, \n",
    "                                                    random_state=100,\n",
    "                                                    update_every=1,\n",
    "                                            \n",
    "                                                    chunksize=100,\n",
    "                                                    passes=10,\n",
    "                                                    alpha='auto',\n",
    "                                                    per_word_topics=True)\n",
    "        print('Coherence score...')\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, \n",
    "                                             texts=data_lemmatized, \n",
    "                                             dictionary=id2word, \n",
    "                                             coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "        print(f'Finished with coherence score {coherence_lda}')\n",
    "        scores.append((n, coherence_lda))\n",
    "        \n",
    "    scores = sorted(scores, key=lambda x: -x[1])\n",
    "    print('* ' * 30)\n",
    "    print(f'Best score: {scores[0][1]} with {scores[0][0]} topics')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * \n",
      "n_topics=5\n",
      "Extracting topics...\n",
      "Coherence score...\n",
      "Finished with coherence score 0.556162379991811\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * \n",
      "n_topics=6\n",
      "Extracting topics...\n",
      "Coherence score...\n",
      "Finished with coherence score 0.5298558581887968\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * \n",
      "n_topics=7\n",
      "Extracting topics...\n",
      "Coherence score...\n",
      "Finished with coherence score 0.5347721693950162\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * \n",
      "n_topics=8\n",
      "Extracting topics...\n",
      "Coherence score...\n",
      "Finished with coherence score 0.4667539140125066\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * \n",
      "n_topics=9\n",
      "Extracting topics...\n",
      "Coherence score...\n",
      "Finished with coherence score 0.529042485995545\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * \n",
      "n_topics=10\n",
      "Extracting topics...\n",
      "Coherence score...\n",
      "Finished with coherence score 0.5047573814189\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * \n",
      "n_topics=15\n",
      "Extracting topics...\n",
      "Coherence score...\n",
      "Finished with coherence score 0.499899150378291\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * \n",
      "n_topics=20\n",
      "Extracting topics...\n",
      "Coherence score...\n",
      "Finished with coherence score 0.49288861569721343\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * \n",
      "n_topics=25\n",
      "Extracting topics...\n",
      "Coherence score...\n",
      "Finished with coherence score 0.4740206007114837\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * \n",
      "n_topics=30\n",
      "Extracting topics...\n",
      "Coherence score...\n",
      "Finished with coherence score 0.46672302875548977\n",
      "* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * \n",
      "Best score: 0.556162379991811 with 5 topics\n"
     ]
    }
   ],
   "source": [
    "topics = [5, 6, 7, 8, 9, 10, 15, 20, 25, 30]\n",
    "\n",
    "best_n = find_best_param(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot([s[0] for s in sorted(best_n)], [s[1] for s in sorted(best_n)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оптимальное число групп равно 5, значит, обучим модель с 5 темами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                    id2word=id2word,\n",
    "                                                    num_topics=5, \n",
    "                                                    random_state=100,\n",
    "                                                    update_every=1,\n",
    "                                                    chunksize=100,\n",
    "                                                    passes=10,\n",
    "                                                    alpha='auto',\n",
    "                                                    per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda_model.show_topics(5, 1000, formatted=False)  # достанем все темы и по 1000 слов к каждой \n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Найдем широкие топики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_words(topic):\n",
    "    \"\"\"\n",
    "    creates a dictionary of \n",
    "    topic words and their weights\n",
    "    \"\"\"\n",
    "    dic = {}     \n",
    "    for word in topic:\n",
    "        dic[word[0]] = float(word[1])\n",
    "    return dic\n",
    "\n",
    "\n",
    "def topics_dict(topics):\n",
    "    \"\"\"\n",
    "    creates a dictionary of topic words\n",
    "    \"\"\"\n",
    "    top_dict = {}\n",
    "    for topic in topics:\n",
    "        top_dict[str(topic[0])] = get_topic_words(topic[1])\n",
    "    return top_dict\n",
    "\n",
    "\n",
    "def find_topic(text, topics):\n",
    "    \"\"\"\n",
    "    returns the most prominent topic \n",
    "    of the text\n",
    "    \"\"\"\n",
    "    text_top = {}\n",
    "    for topic in topics:\n",
    "        for word in text:\n",
    "            if word in topics[topic]:\n",
    "                if topic not in text_top:\n",
    "                    text_top[topic] = 0\n",
    "                text_top[topic] += topics[topic][word]\n",
    "    text_top = sorted(text_top.items(), key=lambda x: -x[1])\n",
    "    # есть пара текстов, где после препроцессинга остается ничего - отсюда None\n",
    "    return text_top[0][0] if len(text_top) >= 1 else None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def classify(data, topics, org_df):\n",
    "    \"\"\"\n",
    "    classifies texts\n",
    "    \"\"\"\n",
    "    # Creating topic dictionary\n",
    "    top_dict = topics_dict(topics)\n",
    "    \n",
    "    df = []\n",
    "    \n",
    "    # Classifying\n",
    "    for i, text in tqdm(enumerate(data)):\n",
    "        topic = find_topic(text, top_dict)\n",
    "        df.append((org_df.loc[i, 'content'], ' '.join(text), topic))\n",
    "    return pd.DataFrame(df, columns=['text', 'lemmas', 'topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500fc98c753746f99fe26d6ac038d48e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "my_df = classify(data_lemmatized, topics, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>where thing car nntp_poste host park line wond...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>si poll final summary final call si clock repo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>question engineering computer network distribu...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>division line host amber write write article k...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>question organization smithsonian_astrophysica...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>From: jim.zisfein@factory.com (Jim Zisfein) \\n...</td>\n",
       "      <td>migraine city ny_bis reply line cheap also wel...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>From: ebodin@pearl.tufts.edu\\nSubject: Screen ...</td>\n",
       "      <td>problem screen blank sometimes minor physical ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>From: westes@netcom.com (Will Estes)\\nSubject:...</td>\n",
       "      <td>este mount case organization mail group line i...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>From: steve@hcrlgw (Steven Collins)\\nSubject: ...</td>\n",
       "      <td>line nntp_poste host article write boy embaras...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>From: gunning@cco.caltech.edu (Kevin J. Gunnin...</td>\n",
       "      <td>gun steal organization line distribution_usa n...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      From: lerxst@wam.umd.edu (where's my thing)\\nS...   \n",
       "1      From: guykuo@carson.u.washington.edu (Guy Kuo)...   \n",
       "2      From: twillis@ec.ecn.purdue.edu (Thomas E Will...   \n",
       "3      From: jgreen@amber (Joe Green)\\nSubject: Re: W...   \n",
       "4      From: jcm@head-cfa.harvard.edu (Jonathan McDow...   \n",
       "...                                                  ...   \n",
       "11309  From: jim.zisfein@factory.com (Jim Zisfein) \\n...   \n",
       "11310  From: ebodin@pearl.tufts.edu\\nSubject: Screen ...   \n",
       "11311  From: westes@netcom.com (Will Estes)\\nSubject:...   \n",
       "11312  From: steve@hcrlgw (Steven Collins)\\nSubject: ...   \n",
       "11313  From: gunning@cco.caltech.edu (Kevin J. Gunnin...   \n",
       "\n",
       "                                                  lemmas topic  \n",
       "0      where thing car nntp_poste host park line wond...     3  \n",
       "1      si poll final summary final call si clock repo...     0  \n",
       "2      question engineering computer network distribu...     3  \n",
       "3      division line host amber write write article k...     3  \n",
       "4      question organization smithsonian_astrophysica...     1  \n",
       "...                                                  ...   ...  \n",
       "11309  migraine city ny_bis reply line cheap also wel...     3  \n",
       "11310  problem screen blank sometimes minor physical ...     0  \n",
       "11311  este mount case organization mail group line i...     3  \n",
       "11312  line nntp_poste host article write boy embaras...     3  \n",
       "11313  gun steal organization line distribution_usa n...     3  \n",
       "\n",
       "[11314 rows x 3 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['3', '0', '1', '2', '4', None], dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df['topic'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF по темам\n",
    "Там не очень понятное задание, как я поняла, для каждой темы считаем tf-idf (то есть корпус - это все тексты, принадлежащие к одной теме)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "\n",
    "def topic_tfidf(texts, topn=500):\n",
    "    vectorizer = TfidfVectorizer(smooth_idf=True, use_idf=True,\n",
    "                                 min_df=0.01, ngram_range=(1,1),\n",
    "                                 stop_words=set([punctuation]+stop_words))\n",
    "    \n",
    "    collection = ' '.join(texts) if type(texts) == list else texts\n",
    "    X = vectorizer.fit_transform([collection])\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    from_sorted=sort_coo(X.tocoo())[:topn]\n",
    "    scores, features = [], []\n",
    "    for idx, score in from_sorted:\n",
    "        scores.append(round(score, 3))\n",
    "        features.append(feature_names[idx])\n",
    "    important_words = {features[idx]: scores[idx] for idx in range(len(features))}\n",
    "    important_words = sorted(important_words.items(), key=itemgetter(1), reverse=True)\n",
    "    return important_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_topics = {}\n",
    "\n",
    "for t in my_df['topic'].unique()[:-1]:\n",
    "    tfidf_topics[t] = {}\n",
    "    texts = my_df[my_df['topic']==t].lemmas.tolist()\n",
    "    top_words = topic_tfidf(texts)\n",
    "    for word, score in top_words:\n",
    "        tfidf_topics[t][word] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc3139f06fb4fe3a4194a9abe57b2e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = []\n",
    "for t in tqdm(my_df['topic'].unique()):\n",
    "    curr_df = my_df[my_df['topic']==t]\n",
    "    for i, text in curr_df.iterrows():\n",
    "        words = []\n",
    "        for word in text['lemmas'].split():\n",
    "            if word in tfidf_topics[t]:\n",
    "                words.append((word, tfidf_topics[t][word]))\n",
    "        words = list(set(sorted(words, key=lambda x: -x[1])))[:5]\n",
    "        words = ', '.join([w[0] for w in words])\n",
    "        new_df.append((text['text'], text['topic'], words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(new_df, columns=['text', 'topic', 'tfidf_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>tfidf_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>3</td>\n",
       "      <td>bring, call, early, really, door</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>3</td>\n",
       "      <td>start, suppose, maybe, news, time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>3</td>\n",
       "      <td>far, address, write, person, really</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: holmes7000@iscsvax.uni.edu\\nSubject: WIn...</td>\n",
       "      <td>3</td>\n",
       "      <td>would, several, mail, line, appreciate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: kerr@ux1.cso.uiuc.edu (Stan Kerr)\\nSubje...</td>\n",
       "      <td>3</td>\n",
       "      <td>owner, however, correct, write, mention</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text topic  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...     3   \n",
       "1  From: twillis@ec.ecn.purdue.edu (Thomas E Will...     3   \n",
       "2  From: jgreen@amber (Joe Green)\\nSubject: Re: W...     3   \n",
       "3  From: holmes7000@iscsvax.uni.edu\\nSubject: WIn...     3   \n",
       "4  From: kerr@ux1.cso.uiuc.edu (Stan Kerr)\\nSubje...     3   \n",
       "\n",
       "                               tfidf_words  \n",
       "0         bring, call, early, really, door  \n",
       "1        start, suppose, maybe, news, time  \n",
       "2      far, address, write, person, really  \n",
       "3   would, several, mail, line, appreciate  \n",
       "4  owner, however, correct, write, mention  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('hw3_gensim_topics.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coherence score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Что такое coherence топика?**\n",
    "    - Связность текстов внутри одной темы, в частности, семантическая соотнесенность слов, выделяемых как самые наиболее вероятные (с наибольшим весом) в теме\n",
    "**Как работает coherence score?**\n",
    "    - у нас есть sliding wondow, своеобразное окно наблюдение с размером 110, которое проходится по частотным словам в топике\n",
    "    - считается co-occurence слов, т.е. частота появления двух терминов из текстового корпуса рядом друг с другом\n",
    "    - затем эти данные используются для подсчета NPMI (упрощенно, вероятности схожести) для пар высокочастотных слов -- получаем вектора для слов\n",
    "    - в конце производится one-set segmentation по самым вероятным словам и в результате получается, что подсчитывается косинусная близость между всеми векторами частотных слов и сумма всех этих векторов, а coherence - это среднее значение всех косинусных близостей. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
